# KoLa: Korean speech to LaTeX extraction Using small LM Modules

ğŸ“¢ 2025ë…„ 1í•™ [AIKU](https://github.com/AIKU-Official) í™œë™ìœ¼ë¡œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤
ğŸ‰ 2025ë…„ 1í•™ê¸° AIKU Conference ì—´ì‹¬íˆìƒ ìˆ˜ìƒ!

## ì†Œê°œ

We introduce KoLa, a lightweight, modular pipeline designed to translate Korean mathematical speech directly into LaTeX expressions. Central to our contribution is KoTeX-100K, KoTeX-400K a newly constructed dataset containing 100,000 aligned triples of Korean mathematical speech audio, textual transcriptions, and corresponding \LaTeX formulas. 

## ë°©ë²•ë¡ 

KoLa utilizes a Whisper-based ASR module followed sequentially by two compact modules: a text-based Error Corrector and a LaTeX Translator. Both modules are independently trained on KoTeX 100K, KoTeX 400K enabling robust correction of ASR errors and flexible improvement of formula transcription quality. Our findings demonstrate the feasibility of high-quality mathematical speech transcription in Korean, highlighting opportunities for extending this approach to multilingual mathematical speech understanding. We publicly release our code and dataset to facilitate further research.

![KoLa Model Architecture](src/model_fig.jpg)


## í™˜ê²½ ì„¤ì •

(Requirements, Anaconda, Docker ë“± í”„ë¡œì íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°ì— í•„ìš”í•œ ìš”êµ¬ ì‚¬í•­ì„ ë‚˜ì—´í•´ì£¼ì„¸ìš”)

## ì‚¬ìš© ë°©ë²•

(í”„ë¡œì íŠ¸ ì‹¤í–‰ ë°©ë²• (ëª…ë ¹ì–´ ë“±)ì„ ì ì–´ì£¼ì„¸ìš”.)

## ì˜ˆì‹œ ê²°ê³¼

- í•œêµ­ì–´ ìŒì„±ì„ ë°›ê³ , ëŒ€ì‘ë˜ëŠ” LaTeXë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

- ì˜ˆì‹œ input: ì—ì´ì œê³± ë”í•˜ê¸° ë¹„ì œê³±
- ì˜ˆì‹œ output:
$$
\frac{a^2 + b^2}
$$

## íŒ€ì›

- [ê¹€ë¯¼ì¤€](ddomjun): build, train, test KoLa model pipeline code
- [ì „í˜œì„œ](doupari): data generation, data pre-/post-processing, evaluation on other LLM models
- [ê¹€íƒœê´€](TTKKWAN): build, train, test KoLa model pipeline code
- [ìœ¤ìŠ¹í˜„](xiseren): data generation, data pre-/post-processing
